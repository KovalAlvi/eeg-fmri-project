{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57334e36-39bf-4342-ab23-5bf0868237b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab4de19-cc35-47c2-912c-d1dc3372c39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/nilearn/input_data/__init__.py:27: FutureWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys, os, json\n",
    "import mne, sklearn, wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from nilearn import datasets, image, masking, plotting\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "\n",
    "\n",
    "# animation part\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# from celluloid import Camera   # it is convinient method to animate\n",
    "from matplotlib import animation, rc\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "## torch libraries \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from pytorch_model_summary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68eacc99-823f-49be-80b1-b432a1833d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "from utils import get_datasets\n",
    "from utils import preproc\n",
    "from utils import torch_dataset\n",
    "from utils import train_utils\n",
    "from utils import inference\n",
    "from utils.models_arch import autoencoder_new, autoencoder_v3_separable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974b520-53af-4aae-8475-ad9864cc86f0",
   "metadata": {},
   "source": [
    "# Set all hyperparameters\n",
    "- Cuda and GPU.\n",
    "- Parameters of dataset. \n",
    "- random seed( if necessary). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9740a7b-6c36-471f-bafe-3361c4758086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 4\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)  # python operation seed\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(torch.cuda.is_available(), torch.cuda.device_count())\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68da9ec-be91-4cc8-b7d2-87c106d53ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(  \n",
    "                dataset_name = 'CWL', # CWL\n",
    "                new_fps=100, \n",
    "                freqs = np.logspace(np.log10(2), np.log10(99), 16), \n",
    "    \n",
    "                n_channels = 30, # 30 \n",
    "                n_roi = 6,\n",
    "                \n",
    "                bold_delay = 5,\n",
    "                to_many = True,\n",
    "                random_subsample = True,\n",
    "                sample_per_epoch = 128, \n",
    "                WINDOW_SIZE = 512,\n",
    "                    \n",
    "                optimizer='adam',\n",
    "                lr=1e-3,\n",
    "                weight_decay=0, \n",
    "                batch_size=32, \n",
    "                \n",
    "                preproc_type = 'dB_log',\n",
    "                loss_function = 'mse', \n",
    "                model_type = 'CorrModel'\n",
    "                )\n",
    "\n",
    "\n",
    "hp_autoencoder = dict(n_electrodes=config['n_channels'],\n",
    "                      n_freqs = len(config['freqs']),\n",
    "                      n_channels_out = config['n_roi'],\n",
    "\n",
    "                     channels=[128, 64, 64, 32], \n",
    "                     kernel_sizes=[15, 11, 5],\n",
    "                     strides=[8, 4, 2], \n",
    "                     # dilation=[1, 1, 1], \n",
    "                     # decoder_reduce=4\n",
    "                     )\n",
    "\n",
    "\n",
    "config = {**hp_autoencoder, **config}\n",
    "\n",
    "params_train = {'batch_size': config['batch_size'],\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0}\n",
    "\n",
    "params_val = {'batch_size': config['batch_size'],\n",
    "              'shuffle': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a1c42-68f1-4afa-9943-02a441b15e07",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Upload preprocessed dataset from np files. \n",
    "It should accelerate speed of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "951efaec-ba20-48c9-94aa-588f3f5b13e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: (30, 16, 20690) (6, 20690)\n",
      "Size of test dataset: (30, 16, 5500) (6, 5500)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/processed/labels_roi_6.json\", 'r') as f:\n",
    "    labels_roi = json.load(f)\n",
    "\n",
    "\n",
    "if config['dataset_name']=='CWL':\n",
    "    dataset_path = '../data/processed/CWL/trio1_100_hz_6_roi_2_99_freqs.npz'\n",
    "    \n",
    "elif config['dataset_name']=='NODDI':\n",
    "    dataset_path = '../data/processed/NODDI/32_100_hz_6_roi_2_99_freqs.npz'\n",
    "else:\n",
    "    print('no such dataset')\n",
    "\n",
    "\n",
    "# download data\n",
    "data = np.load(dataset_path)\n",
    "\n",
    "train_dataset_prep = (data['x_train'], data['y_train'])\n",
    "test_dataset_prep = (data['x_test'], data['y_test'])\n",
    "\n",
    "\n",
    "# apply time dealy corrected\n",
    "train_dataset_prep = preproc.bold_time_delay_align(train_dataset_prep, \n",
    "                                                   config['new_fps'],\n",
    "                                                   config['bold_delay'])\n",
    "test_dataset_prep = preproc.bold_time_delay_align(test_dataset_prep, \n",
    "                                                  config['new_fps'],\n",
    "                                                  config['bold_delay'])\n",
    "\n",
    "\n",
    "print('Size of train dataset:', train_dataset_prep[0].shape, train_dataset_prep[1].shape)\n",
    "print('Size of test dataset:', test_dataset_prep[0].shape, test_dataset_prep[1].shape)\n",
    "\n",
    "# torch dataset creation \n",
    "torch_dataset_train = torch_dataset.CreateDataset_eeg_fmri(train_dataset_prep, \n",
    "                                                            random_sample=config['random_subsample'], \n",
    "                                                            sample_per_epoch=config['sample_per_epoch'], \n",
    "                                                            to_many=config['to_many'], \n",
    "                                                            window_size = config['WINDOW_SIZE'])\n",
    "\n",
    "torch_dataset_test = torch_dataset.CreateDataset_eeg_fmri(test_dataset_prep, \n",
    "                                                            random_sample=False, \n",
    "                                                            sample_per_epoch=None, \n",
    "                                                            to_many=config['to_many'], \n",
    "                                                            window_size = config['WINDOW_SIZE'])\n",
    "\n",
    "# because you do not have strid for val data. \n",
    "torch_dataset_test = Subset(torch_dataset_test, np.arange(len(torch_dataset_test))[::100])\n",
    "\n",
    "# init dataloaders for training\n",
    "train_loader = torch.utils.data.DataLoader(torch_dataset_train, **params_train)\n",
    "val_loader = torch.utils.data.DataLoader(torch_dataset_test, **params_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f97bc-6319-4183-9845-9882afce19d2",
   "metadata": {},
   "source": [
    "## Model investigation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb12696-284c-4d10-9c16-b9a5becd3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class UpsampleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, scale=2):\n",
    "        super(UpsampleConvBlock, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=scale, mode='linear')\n",
    "        self.conv_block = nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size, padding='same'),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.1)\n",
    "                                        # nn.Conv1d(out_channels, out_channels, kernel_size, padding='same'),\n",
    "                                        # nn.ReLU())\n",
    "                                       )\n",
    "                                        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv_block(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class CrossCorrModel(nn.Module):\n",
    "    def __init__(self, n_electrodes=30,\n",
    "                 n_freqs = 16,\n",
    "                 n_channels_out=21, \n",
    "                 window_size = 512):\n",
    "        \n",
    "        super(CrossCorrModel, self).__init__()\n",
    "\n",
    "        self.n_electrodes = n_electrodes\n",
    "        self.n_freqs = n_freqs\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        inp_channels = int(n_freqs * n_electrodes*(n_electrodes-1)/2)\n",
    "        self.project = nn.Sequential(nn.Conv1d(inp_channels, 128, 1),\n",
    "                                     nn.Dropout(p=0.2),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Conv1d(128, 256, 1), \n",
    "                                     nn.Dropout(p=0.2),\n",
    "                                     nn.ReLU())\n",
    "        \n",
    "        self.upsample = nn.Sequential(UpsampleConvBlock(128, 128, kernel_size=3, scale=4),\n",
    "                                      UpsampleConvBlock(128, 128, kernel_size=7, scale=8),\n",
    "                                      UpsampleConvBlock(128, 64, kernel_size=7, scale=8))\n",
    "\n",
    "        self.last = nn.Conv1d(64, 6, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, elec, n_freq, time = x.shape\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x.reshape(batch*n_freq, elec, -1)\n",
    "\n",
    "        x_corrs = torch.stack([torch.corrcoef(x_) for x_ in x])\n",
    "        x_corrs = x_corrs.reshape(batch, n_freq, elec, elec)\n",
    "        \n",
    "        x_corrs = torch.where(torch.abs(x_corrs)>0.3, x_corrs, torch.zeros_like(x_corrs))\n",
    "        \n",
    "        triu_idxs = torch.triu_indices(elec, elec, offset=1)\n",
    "        x_corrs_vec =  x_corrs[..., triu_idxs[0], triu_idxs[1]]\n",
    "        \n",
    "        x_corrs_vec = x_corrs_vec.reshape(batch, -1, 1)\n",
    "        x_corrs_vec = torch.nan_to_num(x_corrs_vec)\n",
    "        \n",
    "        # [bathc, features, 1] -> [[bathc, hidden//4, 4]] reshapping \n",
    "        x_proj = self.project(x_corrs_vec)\n",
    "        x_proj = x_proj.reshape(batch, -1 , 2)\n",
    "        \n",
    "        # generator\n",
    "        x =  self.upsample(x_proj)\n",
    "        x = self.last(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ff0863-786a-4c24-9130-c997833869c1",
   "metadata": {},
   "source": [
    "# Init Model, Loss, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d974306a-4015-440a-8370-9e2eba5b6348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "          Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "===========================================================================\n",
      "              Conv1d-1        [4, 6960, 1]         891,008         891,008\n",
      "             Dropout-2         [4, 128, 1]               0               0\n",
      "                ReLU-3         [4, 128, 1]               0               0\n",
      "              Conv1d-4         [4, 128, 1]          33,024          33,024\n",
      "             Dropout-5         [4, 256, 1]               0               0\n",
      "                ReLU-6         [4, 256, 1]               0               0\n",
      "   UpsampleConvBlock-7         [4, 128, 2]          49,280          49,280\n",
      "   UpsampleConvBlock-8         [4, 128, 8]         114,816         114,816\n",
      "   UpsampleConvBlock-9        [4, 128, 64]          57,408          57,408\n",
      "             Conv1d-10        [4, 64, 512]             390             390\n",
      "===========================================================================\n",
      "Total params: 1,145,926\n",
      "Trainable params: 1,145,926\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = CrossCorrModel(n_electrodes=config['n_electrodes'],\n",
    "                 n_freqs = config['n_freqs'],\n",
    "                 n_channels_out=6, \n",
    "                 window_size=512)\n",
    "print(summary(model, torch.zeros(4, config['n_channels'], \n",
    "                                 len(config['freqs']),\n",
    "                                 config['WINDOW_SIZE']).float(), show_input=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e4982-344f-4d44-8d7e-445f4c4a5662",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d21727-423a-4872-8bec-82fbd3b4fb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkoval_alvi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/koval_alvi/eeg_fmri/runs/1hgrlepe\" target=\"_blank\">custard-meringue-229</a></strong> to <a href=\"https://wandb.ai/koval_alvi/eeg_fmri\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_electrodes': 30, 'n_freqs': 16, 'n_channels_out': 6, 'channels': [128, 64, 64, 32], 'kernel_sizes': [15, 11, 5], 'strides': [8, 4, 2], 'dataset_name': 'CWL', 'new_fps': 100, 'freqs': array([ 2.        ,  2.59420132,  3.36494024,  4.3646662 ,  5.6614114 ,\n",
      "        7.34342046,  9.52515552, 12.3550855 , 16.02578954, 20.78706217,\n",
      "       26.96291204, 34.97361097, 45.36429384, 58.84205542, 76.32406886,\n",
      "       99.        ]), 'n_channels': 30, 'n_roi': 6, 'bold_delay': 5, 'to_many': True, 'random_subsample': True, 'sample_per_epoch': 128, 'WINDOW_SIZE': 512, 'optimizer': 'adam', 'lr': 0.001, 'weight_decay': 0, 'batch_size': 32, 'preproc_type': 'dB_log', 'loss_function': 'mse', 'model_type': 'CorrModel'}\n",
      "CrossCorrModel(\n",
      "  (project): Sequential(\n",
      "    (0): Conv1d(6960, 128, kernel_size=(1,), stride=(1,))\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (upsample): Sequential(\n",
      "    (0): UpsampleConvBlock(\n",
      "      (upsample): Upsample(scale_factor=4.0, mode=linear)\n",
      "      (conv_block): Sequential(\n",
      "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): UpsampleConvBlock(\n",
      "      (upsample): Upsample(scale_factor=8.0, mode=linear)\n",
      "      (conv_block): Sequential(\n",
      "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=same)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): UpsampleConvBlock(\n",
      "      (upsample): Upsample(scale_factor=8.0, mode=linear)\n",
      "      (conv_block): Sequential(\n",
      "        (0): Conv1d(128, 64, kernel_size=(7,), stride=(1,), padding=same)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (last): Conv1d(64, 6, kernel_size=(1,), stride=(1,))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "          Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "===========================================================================\n",
      "              Conv1d-1        [4, 6960, 1]         891,008         891,008\n",
      "             Dropout-2         [4, 128, 1]               0               0\n",
      "                ReLU-3         [4, 128, 1]               0               0\n",
      "              Conv1d-4         [4, 128, 1]          33,024          33,024\n",
      "             Dropout-5         [4, 256, 1]               0               0\n",
      "                ReLU-6         [4, 256, 1]               0               0\n",
      "   UpsampleConvBlock-7         [4, 128, 2]          49,280          49,280\n",
      "   UpsampleConvBlock-8         [4, 128, 8]         114,816         114,816\n",
      "   UpsampleConvBlock-9        [4, 128, 64]          57,408          57,408\n",
      "             Conv1d-10        [4, 64, 512]             390             390\n",
      "===========================================================================\n",
      "Total params: 1,145,926\n",
      "Trainable params: 1,145,926\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Starting Training of our model \n",
      "Number of samples 128 \n",
      "Size of batch: 32 Number batches 4\n",
      "....."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "Epoch 5 train loss_0 : 0.975 val loss_0 : 1.32 train loss_1 : 0.00383 val loss_1 : 0.0109 \n",
      "........."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      "Epoch 10 train loss_0 : 0.928 val loss_0 : 1.33 train loss_1 : 0.0158 val loss_1 : 0.0754 \n",
      "....................\n",
      "Epoch 15 train loss_0 : 0.849 val loss_0 : 1.35 train loss_1 : 0.0438 val loss_1 : 0.0917 \n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................\n",
      "Epoch 20 train loss_0 : 0.818 val loss_0 : 1.28 train loss_1 : 0.105 val loss_1 : 0.0933 \n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........\n",
      "Epoch 25 train loss_0 : 0.729 val loss_0 : 1.27 train loss_1 : 0.188 val loss_1 : 0.24 \n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "Epoch 30 train loss_0 : 0.65 val loss_0 : 1.26 train loss_1 : 0.31 val loss_1 : 0.256 \n",
      "....."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      "Epoch 35 train loss_0 : 0.58 val loss_0 : 1.26 train loss_1 : 0.347 val loss_1 : 0.156 \n",
      "....................\n",
      "Epoch 40 train loss_0 : 0.556 val loss_0 : 1.22 train loss_1 : 0.353 val loss_1 : 0.202 \n",
      "....................\n",
      "Epoch 45 train loss_0 : 0.524 val loss_0 : 1.24 train loss_1 : 0.388 val loss_1 : 0.156 \n",
      "............."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      "Epoch 50 train loss_0 : 0.492 val loss_0 : 1.15 train loss_1 : 0.453 val loss_1 : 0.338 \n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      "Epoch 55 train loss_0 : 0.447 val loss_0 : 1.23 train loss_1 : 0.515 val loss_1 : 0.248 \n",
      "....................\n",
      "Epoch 60 train loss_0 : 0.468 val loss_0 : 1.18 train loss_1 : 0.441 val loss_1 : 0.24 \n",
      "....."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      "Epoch 65 train loss_0 : 0.406 val loss_0 : 1.16 train loss_1 : 0.509 val loss_1 : 0.226 \n",
      "....................\n",
      "Epoch 70 train loss_0 : 0.461 val loss_0 : 1.23 train loss_1 : 0.429 val loss_1 : 0.235 \n",
      "................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv_torch/lib/python3.9/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "Epoch 75 train loss_0 : 0.482 val loss_0 : 1.34 train loss_1 : 0.506 val loss_1 : 0.136 \n",
      "....................\n",
      "Epoch 80 train loss_0 : 0.457 val loss_0 : 1.2 train loss_1 : 0.439 val loss_1 : 0.241 \n",
      "....................\n",
      "Epoch 85 train loss_0 : 0.449 val loss_0 : 1.19 train loss_1 : 0.501 val loss_1 : 0.208 \n",
      "....................\n",
      "Epoch 90 train loss_0 : 0.387 val loss_0 : 1.24 train loss_1 : 0.514 val loss_1 : 0.308 \n",
      "....................\n",
      "Epoch 95 train loss_0 : 0.401 val loss_0 : 1.18 train loss_1 : 0.512 val loss_1 : 0.192 \n",
      "....................\n",
      "Epoch 100 train loss_0 : 0.397 val loss_0 : 1.15 train loss_1 : 0.554 val loss_1 : 0.283 \n",
      "....................\n",
      "Epoch 105 train loss_0 : 0.397 val loss_0 : 1.15 train loss_1 : 0.534 val loss_1 : 0.264 \n",
      "....................\n",
      "Epoch 110 train loss_0 : 0.372 val loss_0 : 1.16 train loss_1 : 0.535 val loss_1 : 0.322 \n",
      "....................\n",
      "Epoch 115 train loss_0 : 0.373 val loss_0 : 1.17 train loss_1 : 0.524 val loss_1 : 0.286 \n",
      "....................\n",
      "Epoch 120 train loss_0 : 0.364 val loss_0 : 1.24 train loss_1 : 0.559 val loss_1 : 0.3 \n",
      "....................\n",
      "Epoch 125 train loss_0 : 0.379 val loss_0 : 1.23 train loss_1 : 0.557 val loss_1 : 0.217 \n",
      "....................\n",
      "Epoch 130 train loss_0 : 0.411 val loss_0 : 1.26 train loss_1 : 0.547 val loss_1 : 0.183 \n",
      "....................\n",
      "Epoch 135 train loss_0 : 0.362 val loss_0 : 1.26 train loss_1 : 0.588 val loss_1 : 0.225 \n",
      "....................\n",
      "Epoch 140 train loss_0 : 0.355 val loss_0 : 1.15 train loss_1 : 0.555 val loss_1 : 0.3 \n",
      "....................\n",
      "Epoch 145 train loss_0 : 0.376 val loss_0 : 1.21 train loss_1 : 0.559 val loss_1 : 0.225 \n",
      "....................\n",
      "Epoch 150 train loss_0 : 0.39 val loss_0 : 1.17 train loss_1 : 0.562 val loss_1 : 0.277 \n",
      "....................\n",
      "Epoch 155 train loss_0 : 0.336 val loss_0 : 1.25 train loss_1 : 0.6 val loss_1 : 0.294 \n",
      "....................\n",
      "Epoch 160 train loss_0 : 0.341 val loss_0 : 1.2 train loss_1 : 0.535 val loss_1 : 0.301 \n",
      "....................\n",
      "Epoch 165 train loss_0 : 0.37 val loss_0 : 1.16 train loss_1 : 0.537 val loss_1 : 0.302 \n",
      "....................\n",
      "Epoch 170 train loss_0 : 0.311 val loss_0 : 1.24 train loss_1 : 0.612 val loss_1 : 0.232 \n",
      "....................\n",
      "Epoch 175 train loss_0 : 0.313 val loss_0 : 1.26 train loss_1 : 0.614 val loss_1 : 0.188 \n",
      "....................\n",
      "Epoch 180 train loss_0 : 0.313 val loss_0 : 1.26 train loss_1 : 0.589 val loss_1 : 0.258 \n",
      "....................\n",
      "Epoch 185 train loss_0 : 0.312 val loss_0 : 1.25 train loss_1 : 0.629 val loss_1 : 0.303 \n",
      "....................\n",
      "Epoch 190 train loss_0 : 0.34 val loss_0 : 1.19 train loss_1 : 0.615 val loss_1 : 0.248 \n",
      "....................\n",
      "Epoch 195 train loss_0 : 0.379 val loss_0 : 1.24 train loss_1 : 0.574 val loss_1 : 0.295 \n",
      "....................\n",
      "Epoch 200 train loss_0 : 0.319 val loss_0 : 1.24 train loss_1 : 0.594 val loss_1 : 0.249 \n",
      "....................\n",
      "Epoch 205 train loss_0 : 0.321 val loss_0 : 1.2 train loss_1 : 0.61 val loss_1 : 0.279 \n",
      "....................\n",
      "Epoch 210 train loss_0 : 0.344 val loss_0 : 1.24 train loss_1 : 0.625 val loss_1 : 0.265 \n",
      "....................\n",
      "Epoch 215 train loss_0 : 0.325 val loss_0 : 1.24 train loss_1 : 0.616 val loss_1 : 0.262 \n",
      "....................\n",
      "Epoch 220 train loss_0 : 0.322 val loss_0 : 1.23 train loss_1 : 0.644 val loss_1 : 0.281 \n",
      "....................\n",
      "Epoch 225 train loss_0 : 0.316 val loss_0 : 1.21 train loss_1 : 0.622 val loss_1 : 0.309 \n",
      "....................\n",
      "Epoch 230 train loss_0 : 0.32 val loss_0 : 1.21 train loss_1 : 0.649 val loss_1 : 0.315 \n",
      "....................\n",
      "Epoch 235 train loss_0 : 0.309 val loss_0 : 1.2 train loss_1 : 0.615 val loss_1 : 0.287 \n",
      "....................\n",
      "Epoch 240 train loss_0 : 0.328 val loss_0 : 1.2 train loss_1 : 0.59 val loss_1 : 0.273 \n",
      "....................\n",
      "Epoch 245 train loss_0 : 0.332 val loss_0 : 1.22 train loss_1 : 0.641 val loss_1 : 0.296 \n",
      "....................\n",
      "Epoch 250 train loss_0 : 0.306 val loss_0 : 1.27 train loss_1 : 0.635 val loss_1 : 0.246 \n",
      "....................\n",
      "Epoch 255 train loss_0 : 0.266 val loss_0 : 1.25 train loss_1 : 0.673 val loss_1 : 0.269 \n",
      "....................\n",
      "Epoch 260 train loss_0 : 0.321 val loss_0 : 1.24 train loss_1 : 0.614 val loss_1 : 0.259 \n",
      "....................\n",
      "Epoch 265 train loss_0 : 0.301 val loss_0 : 1.34 train loss_1 : 0.598 val loss_1 : 0.195 \n",
      "....................\n",
      "Epoch 270 train loss_0 : 0.345 val loss_0 : 1.31 train loss_1 : 0.593 val loss_1 : 0.158 \n",
      "....................\n",
      "Epoch 275 train loss_0 : 0.307 val loss_0 : 1.19 train loss_1 : 0.667 val loss_1 : 0.291 \n",
      "....................\n",
      "Epoch 280 train loss_0 : 0.332 val loss_0 : 1.22 train loss_1 : 0.583 val loss_1 : 0.223 \n",
      "....................\n",
      "Epoch 285 train loss_0 : 0.338 val loss_0 : 1.24 train loss_1 : 0.622 val loss_1 : 0.286 \n",
      "....................\n",
      "Epoch 290 train loss_0 : 0.321 val loss_0 : 1.19 train loss_1 : 0.6 val loss_1 : 0.253 \n",
      "....................\n",
      "Epoch 295 train loss_0 : 0.321 val loss_0 : 1.21 train loss_1 : 0.643 val loss_1 : 0.288 \n",
      "....................\n",
      "Epoch 300 train loss_0 : 0.303 val loss_0 : 1.19 train loss_1 : 0.698 val loss_1 : 0.248 \n",
      "....................\n",
      "Epoch 305 train loss_0 : 0.291 val loss_0 : 1.22 train loss_1 : 0.632 val loss_1 : 0.27 \n",
      "....................\n",
      "Epoch 310 train loss_0 : 0.316 val loss_0 : 1.25 train loss_1 : 0.638 val loss_1 : 0.194 \n",
      "....................\n",
      "Epoch 315 train loss_0 : 0.31 val loss_0 : 1.19 train loss_1 : 0.631 val loss_1 : 0.292 \n",
      "....................\n",
      "Epoch 320 train loss_0 : 0.333 val loss_0 : 1.27 train loss_1 : 0.618 val loss_1 : 0.209 \n",
      "....................\n",
      "Epoch 325 train loss_0 : 0.315 val loss_0 : 1.23 train loss_1 : 0.644 val loss_1 : 0.288 \n",
      "....................\n",
      "Epoch 330 train loss_0 : 0.251 val loss_0 : 1.19 train loss_1 : 0.651 val loss_1 : 0.28 \n",
      "....................\n",
      "Epoch 335 train loss_0 : 0.307 val loss_0 : 1.19 train loss_1 : 0.622 val loss_1 : 0.29 \n",
      "....................\n",
      "Epoch 340 train loss_0 : 0.27 val loss_0 : 1.24 train loss_1 : 0.675 val loss_1 : 0.221 \n",
      "....................\n",
      "Epoch 345 train loss_0 : 0.302 val loss_0 : 1.23 train loss_1 : 0.64 val loss_1 : 0.264 \n",
      "....................\n",
      "Epoch 350 train loss_0 : 0.294 val loss_0 : 1.22 train loss_1 : 0.658 val loss_1 : 0.286 \n",
      "....................\n",
      "Epoch 355 train loss_0 : 0.32 val loss_0 : 1.2 train loss_1 : 0.624 val loss_1 : 0.263 \n",
      "....................\n",
      "Epoch 360 train loss_0 : 0.317 val loss_0 : 1.19 train loss_1 : 0.618 val loss_1 : 0.241 \n",
      "....................\n",
      "Epoch 365 train loss_0 : 0.305 val loss_0 : 1.23 train loss_1 : 0.662 val loss_1 : 0.251 \n",
      "....................\n",
      "Epoch 370 train loss_0 : 0.303 val loss_0 : 1.22 train loss_1 : 0.661 val loss_1 : 0.279 \n",
      "....................\n",
      "Epoch 375 train loss_0 : 0.276 val loss_0 : 1.23 train loss_1 : 0.589 val loss_1 : 0.202 \n",
      "....................\n",
      "Epoch 380 train loss_0 : 0.293 val loss_0 : 1.21 train loss_1 : 0.628 val loss_1 : 0.305 \n",
      "....................\n",
      "Epoch 385 train loss_0 : 0.27 val loss_0 : 1.21 train loss_1 : 0.665 val loss_1 : 0.273 \n",
      "....................\n",
      "Epoch 390 train loss_0 : 0.276 val loss_0 : 1.25 train loss_1 : 0.58 val loss_1 : 0.268 \n",
      "....................\n",
      "Epoch 395 train loss_0 : 0.332 val loss_0 : 1.2 train loss_1 : 0.619 val loss_1 : 0.28 \n",
      "....................\n",
      "Epoch 400 train loss_0 : 0.291 val loss_0 : 1.26 train loss_1 : 0.618 val loss_1 : 0.236 \n",
      "....................\n",
      "Epoch 405 train loss_0 : 0.319 val loss_0 : 1.26 train loss_1 : 0.656 val loss_1 : 0.259 \n",
      "....................\n",
      "Epoch 410 train loss_0 : 0.283 val loss_0 : 1.27 train loss_1 : 0.661 val loss_1 : 0.244 \n",
      "....................\n",
      "Epoch 415 train loss_0 : 0.29 val loss_0 : 1.25 train loss_1 : 0.677 val loss_1 : 0.252 \n",
      "....................\n",
      "Epoch 420 train loss_0 : 0.272 val loss_0 : 1.25 train loss_1 : 0.653 val loss_1 : 0.226 \n",
      "....................\n",
      "Epoch 425 train loss_0 : 0.271 val loss_0 : 1.25 train loss_1 : 0.688 val loss_1 : 0.236 \n",
      "....................\n",
      "Epoch 430 train loss_0 : 0.264 val loss_0 : 1.23 train loss_1 : 0.709 val loss_1 : 0.237 \n",
      "....................\n",
      "Epoch 435 train loss_0 : 0.293 val loss_0 : 1.21 train loss_1 : 0.67 val loss_1 : 0.278 \n",
      "....................\n",
      "Epoch 440 train loss_0 : 0.263 val loss_0 : 1.24 train loss_1 : 0.648 val loss_1 : 0.279 \n",
      "....................\n",
      "Epoch 445 train loss_0 : 0.269 val loss_0 : 1.28 train loss_1 : 0.673 val loss_1 : 0.244 \n",
      "....................\n",
      "Epoch 450 train loss_0 : 0.306 val loss_0 : 1.25 train loss_1 : 0.618 val loss_1 : 0.247 \n",
      "....................\n",
      "Epoch 455 train loss_0 : 0.27 val loss_0 : 1.24 train loss_1 : 0.723 val loss_1 : 0.257 \n",
      "....................\n",
      "Epoch 460 train loss_0 : 0.273 val loss_0 : 1.26 train loss_1 : 0.666 val loss_1 : 0.271 \n",
      "....................\n",
      "Epoch 465 train loss_0 : 0.266 val loss_0 : 1.27 train loss_1 : 0.701 val loss_1 : 0.256 \n",
      "....................\n",
      "Epoch 470 train loss_0 : 0.288 val loss_0 : 1.23 train loss_1 : 0.681 val loss_1 : 0.272 \n",
      "....................\n",
      "Epoch 475 train loss_0 : 0.278 val loss_0 : 1.26 train loss_1 : 0.678 val loss_1 : 0.273 \n",
      "....................\n",
      "Epoch 480 train loss_0 : 0.307 val loss_0 : 1.25 train loss_1 : 0.646 val loss_1 : 0.281 \n",
      "....................\n",
      "Epoch 485 train loss_0 : 0.288 val loss_0 : 1.27 train loss_1 : 0.671 val loss_1 : 0.235 \n",
      "....................\n",
      "Epoch 490 train loss_0 : 0.259 val loss_0 : 1.22 train loss_1 : 0.695 val loss_1 : 0.246 \n",
      "....................\n",
      "Epoch 495 train loss_0 : 0.261 val loss_0 : 1.29 train loss_1 : 0.68 val loss_1 : 0.247 \n",
      "....................\n",
      "Epoch 500 train loss_0 : 0.252 val loss_0 : 1.23 train loss_1 : 0.699 val loss_1 : 0.281 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18870... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 3.18MB of 3.18MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/loss_0</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>train/loss_1</td><td>▁▂▃▄▅▆▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇█▇▇▇▇███▇██</td></tr><tr><td>val/corr_mean</td><td>▂▃▃▄▃▁▃▃▂▄▄▄▇▆▇██</td></tr><tr><td>val/loss_0</td><td>▆█▆▃▅▂▃▂▃▂▄▁▃▁▅▆▄▃▅▃▄▅▃▅▃▅▃▃▂▂▄▅▄▄▅▄▅▅▄▄</td></tr><tr><td>val/loss_1</td><td>▁▁▅▄▃█▅▅▅▅▆▇▆▆▆▇▆▆▆▇▅▆▅▆▆▅▇▆▅▆▆▅▅▅▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/loss_0</td><td>0.25174</td></tr><tr><td>train/loss_1</td><td>0.69885</td></tr><tr><td>val/loss_0</td><td>1.22658</td></tr><tr><td>val/loss_1</td><td>0.28057</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 34 media file(s), 0 artifact file(s) and 2 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">custard-meringue-229</strong>: <a href=\"https://wandb.ai/koval_alvi/eeg_fmri/runs/1hgrlepe\" target=\"_blank\">https://wandb.ai/koval_alvi/eeg_fmri/runs/1hgrlepe</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220314_165359-1hgrlepe/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_runs = 1\n",
    "\n",
    "for i in range(n_runs):\n",
    "    \n",
    "    model = CrossCorrModel(n_electrodes=config['n_electrodes'],\n",
    "                     n_freqs = config['n_freqs'],\n",
    "                     n_channels_out=6,\n",
    "                     window_size=512)\n",
    "    \n",
    "    loss_func = train_utils.make_mse_loss()\n",
    "    train_step = train_utils.train_step\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=config['lr'], \n",
    "                       weight_decay=config['weight_decay'])\n",
    "    \n",
    "    \n",
    "    parameters = {\n",
    "        'EPOCHS': 500,\n",
    "        'model': model, \n",
    "        'train_loader': train_loader, \n",
    "        'val_loader': val_loader, \n",
    "        'loss_function': loss_func,\n",
    "        'train_step': train_step,\n",
    "        'optimizer': optimizer, \n",
    "        'device': 'cuda', \n",
    "        'raw_test_data': test_dataset_prep,\n",
    "        'show_info': 5, \n",
    "        'num_losses': 5,\n",
    "        'labels': labels_roi,\n",
    "        'inference_function': inference.model_inference_function, \n",
    "        'to_many': config['to_many']\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    path_to_save_wandb = 'common/koval_alvi/Checkpoints/wandb_brain'\n",
    "    \n",
    "    \n",
    "    with wandb.init(project=\"eeg_fmri\", config=config, save_code=True):\n",
    "        \n",
    "        wandb.define_metric(\"val/corr_mean\", summary=\"max\")\n",
    "\n",
    "        if i == 0: \n",
    "            exp_name = wandb.run.name\n",
    "        \n",
    "        wandb.run.name = exp_name +'_run_' + str(i)\n",
    "        \n",
    "        print(config)\n",
    "        print(parameters['model'])\n",
    "        print(summary(model, torch.zeros(4, config['n_channels'],\n",
    "                                         len(config['freqs']), config['WINDOW_SIZE']), show_input=True))\n",
    "        \n",
    "        model = train_utils.wanb_train_regression(**parameters)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923ea3f-b6c9-495e-bea2-36552a50cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e40a09-12c2-43b9-957f-8b37d0a0daa8",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_torch",
   "language": "python",
   "name": "myenv_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
