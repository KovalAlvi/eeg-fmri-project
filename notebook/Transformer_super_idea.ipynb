{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6b2c92-ed9d-4743-8d8a-a780b42d24af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interpretable Transformer-based architecture.\n",
    "\n",
    "Pretrain via MAE: -> https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/mae.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cda5312-c3b4-43df-a03c-1f7ed4ed8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.9/site-packages/nilearn/datasets/__init__.py:93: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "\n",
    "import sklearn\n",
    "import mne\n",
    "import wandb\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "\n",
    "\n",
    "from nilearn import image\n",
    "from nilearn import plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from nilearn import datasets, image, masking\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "\n",
    "\n",
    "# animation part\n",
    "from IPython.display import HTML\n",
    "# from celluloid import Camera   # it is convinient method to animate\n",
    "from matplotlib import animation, rc\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "\n",
    "## torch libraries \n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from pytorch_model_summary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe56de9d-e243-44d9-ad57-ed8880cc24a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97377e15-d4f7-43c6-8bd9-b9a8d571ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Input is [batch, emb, time]\n",
    "    simple conv block from wav2vec 2.0 \n",
    "        - conv\n",
    "        - layer norm by embedding axis\n",
    "        - activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1, dilation=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        \n",
    "        # use it instead stride. \n",
    "        self.downsample = nn.AvgPool1d(kernel_size=stride, stride=stride)\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, \n",
    "                                kernel_size=kernel_size, \n",
    "                                bias=False, \n",
    "                                padding='same')\n",
    "        \n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.downsample(x)\n",
    "        x = self.conv1d(x)\n",
    "        \n",
    "        # norm by last axis.\n",
    "        x = torch.transpose(x, -2, -1) \n",
    "        x = self.norm(x)\n",
    "        x = torch.transpose(x, -2, -1) \n",
    "        \n",
    "        \n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class wav2vec_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Extract some features from one time serias as raw speech.\n",
    "    \n",
    "    To do make it possibl;e to work with raw EEG electrode recording. \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_inp_features=30,\n",
    "                 channels = [32, 32, 32], \n",
    "                 kernel_sizes=[3, 3, 3],\n",
    "                 strides=[2, 2, 2], \n",
    "                 dilations = [1, 1, 1]):\n",
    "        \n",
    "        super(wav2vec_conv, self).__init__()\n",
    "        \n",
    "        # freqs-electrodes  reducing to channels[0].\n",
    "        # add additional layer\n",
    "        channels = [n_inp_features] + channels\n",
    "        \n",
    "        self.model_depth = len(channels)-1\n",
    "        # create downsample blcoks in Sequentional manner.\n",
    "        self.downsample_blocks = nn.ModuleList([Conv_block(channels[i], \n",
    "                                                        channels[i+1], \n",
    "                                                        kernel_sizes[i],\n",
    "                                                        stride=strides[i], \n",
    "                                                        dilation=dilations[i]) for i in range(self.model_depth)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. Encode  \n",
    "        \"\"\"\n",
    "        batch, n_freq, time = x.shape\n",
    "    \n",
    "        # encode information\n",
    "        for block  in self.downsample_blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e0e4de-bf7f-4599-97df-f117a252103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec_aggregate(nn.Module):\n",
    "    \"\"\"\n",
    "    Inpy should be. \n",
    "    batch, n_ch, n_freq, time = x.shape\n",
    "    \n",
    "    model_embedding - should be work with [batch, n_freqs, time]\n",
    "    \n",
    "    Return [batch, n_ch, emb, time//stride] \n",
    "    \"\"\"\n",
    "    def __init__(self, model_embedding):\n",
    "        \n",
    "        super(Wav2Vec_aggregate, self).__init__()\n",
    "        \n",
    "        # freqs-electrodes  reducing to channels[0].\n",
    "        channels = [n_inp_features] + channels\n",
    "        \n",
    "        # create downsample blcoks in Sequentional manner.\n",
    "        self.embedding = model_embedding\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. Encode  \n",
    "        \"\"\"\n",
    "        batch, n_ch, n_freq, time = x.shape\n",
    "        \n",
    "        emb_list  = []\n",
    "        for ch in range(n_ch):\n",
    "            x_ch = self.embedding(x[:, ch])\n",
    "            \n",
    "            emb_list.append(x_emb)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b534a-e43b-4979-8c06-234421d0d195",
   "metadata": {},
   "source": [
    "# Extract features from EEG \n",
    "- Raw signal \n",
    "    - apply wav2net encoder for each electrode separately \n",
    "- Wavelet features \n",
    "    - Apply wav2net separately for better features extraction \n",
    "    - Simple downsampling temporal part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d795ff0e-da97-494e-a5eb-39ce70f105fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla transformer aggregate all \n",
    "    [batch, n_electrodes, embed_dim, time]\n",
    "    \n",
    "    Input of transformer is -> [batch, sequence_length, embed_dim]\n",
    "    \n",
    "    Return \n",
    "        [batch, 21]\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_electrodes = 64,\n",
    "                 sequence_length = 128,\n",
    "                 embed_dim=256,\n",
    "                 n_roi=21,\n",
    "                 num_heads=4,\n",
    "                 mlp_ratio=2,\n",
    "                 attn_dropout=0.1,\n",
    "                 num_layers=1,                \n",
    "                 mlp_dropout=0.1,\n",
    "                ):\n",
    "        super(Vanilla_transformer, self).__init__()\n",
    "        self.n_electrodes = n_electrodes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.class_embed = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        \n",
    "        # just add vector \n",
    "        self.pe = nn.Parameter(torch.zeros(1, n_electrodes*sequence_length + 1, embed_dim), requires_grad=True)\n",
    "        \n",
    "        # less parameters. factorises pe.\n",
    "\n",
    "        self.pe_spatial = nn.Parameter(torch.zeros(1, n_electrodes, 1, embed_dim), requires_grad=True)\n",
    "        self.pe_temporal = nn.Parameter(torch.zeros(1, 1, sequence_length , embed_dim), requires_grad=True)\n",
    "        \n",
    "        self.pe_merged = self.pe_spatial + self.pe_temporal\n",
    "        self.pe_merged = self.pe_merged.reshape(1, n_electrodes*sequence_length, embed_dim)\n",
    "        self.pe_merged = torch.cat((torch.zeros(1, 1, embed_dim), self.pe_merged), dim= 1)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                            nhead=num_heads, \n",
    "                                                            dim_feedforward=embed_dim*mlp_ratio, \n",
    "                                                            dropout=attn_dropout, \n",
    "                                                            activation='relu', \n",
    "                                                            batch_first=True)\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        # take our vector and make 21 prediction. \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*mlp_ratio),\n",
    "            nn.Dropout(p=mlp_dropout), \n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim*mlp_ratio, n_roi))\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x.shape = > [batch, n_electrodes, embed_dim, time]\n",
    "        \n",
    "        \"\"\"\n",
    "        print('Input size: ', x.shape)\n",
    "        batch = x.shape[0]\n",
    "        x = x.transpose(2, 3)\n",
    "        # print('After reshape: ', x.shape)\n",
    "        # print(self.sequence_length , self.embed_dim, self.embed_dim)\n",
    "        x = x.reshape(batch, self.sequence_length * self.n_electrodes, self.embed_dim)\n",
    "        print('After reshape: ', x.shape)\n",
    "        \n",
    "        # repeating like batch size and add seg lenght. \n",
    "        class_token = self.class_embed.expand(batch, -1, -1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "        \n",
    "        # add positional embeddings [batch, sequence_length, embed_dim]\n",
    "        x += self.pe_merged\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # take first vector -> normalize -> mlp into \n",
    "        x_cls = x[:, 0]\n",
    "        x_cls = self.mlp_head(self.norm(x_cls))\n",
    "        return x_cls\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e13acc1a-ab02-4da0-9d08-ef04f6b3103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Factorized_transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla transformer aggregate all \n",
    "    [batch, n_electrodes, embed_dim, time]\n",
    "    \n",
    "    Input of transformer is -> [batch, sequence_length, embed_dim]\n",
    "    \n",
    "    \n",
    "    \n",
    "    Spatial transformer ( electrode aggregation ):\n",
    "        [batch, n_electrodes, embed_dim, time] -> [batch, n_roi, embed_dim, time]\n",
    "    Temporal transformer: (time aggregation) \n",
    "        [batch, n_roi, embed_dim, time] -> [batch, n_roi, embed_dim]\n",
    "    Prediction\n",
    "         [batch, n_roi, embed_dim] -> [batch, n_roi]\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    Return \n",
    "        [batch, 21]\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_electrodes = 64,\n",
    "                 sequence_length = 128,\n",
    "                 embed_dim=256,\n",
    "                 n_roi=21,\n",
    "                 num_heads=4,\n",
    "                 mlp_ratio=2,\n",
    "                 attn_dropout=0.1,\n",
    "                 num_layers_spatial=1,\n",
    "                 num_layers_temporal=1,\n",
    "                 mlp_dropout=0.1,\n",
    "                ):\n",
    "        super(Factorized_transformer, self).__init__()\n",
    "        self.n_electrodes = n_electrodes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_roi = n_roi\n",
    "        \n",
    "                \n",
    "        self.roi_tokens = nn.Parameter(torch.zeros(1, n_roi, embed_dim), requires_grad=True)\n",
    "        self.time_tokens = nn.Parameter(torch.zeros(1, n_roi, 1, embed_dim), requires_grad=True) # use different reg tokent for each roi.\n",
    "   \n",
    "\n",
    "        self.pe_spatial = nn.Parameter(torch.zeros(1, n_roi + n_electrodes, embed_dim), requires_grad=True)\n",
    "        self.pe_temporal = nn.Parameter(torch.zeros(1, 1 + sequence_length, embed_dim), requires_grad=True)\n",
    "        \n",
    "        \n",
    "        spatial_layer = nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                                nhead=num_heads, \n",
    "                                                                dim_feedforward=embed_dim*mlp_ratio, \n",
    "                                                                dropout=attn_dropout, \n",
    "                                                                activation='relu', \n",
    "                                                                batch_first=True)\n",
    "        \n",
    "        temporal_layer = nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                            nhead=num_heads, \n",
    "                                                            dim_feedforward=embed_dim*mlp_ratio, \n",
    "                                                            dropout=attn_dropout, \n",
    "                                                            activation='relu', \n",
    "                                                            batch_first=True)\n",
    "        \n",
    "        self.spatial_transformer = nn.TransformerEncoder(spatial_layer, num_layers=num_layers_spatial)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(temporal_layer, num_layers=num_layers_temporal)\n",
    "\n",
    "\n",
    "\n",
    "#         self.mlp_heads = nn.ModuleList([nn.Sequential(\n",
    "#                                         nn.LayerNorm(embed_dim),\n",
    "#                                         nn.Linear(embed_dim, embed_dim*mlp_ratio),\n",
    "#                                         nn.Dropout(p=mlp_dropout), \n",
    "#                                         nn.GELU(),\n",
    "#                                         nn.Linear(embed_dim*mlp_ratio, 1)\n",
    "#                                         ) for i in range(self.n_roi)])\n",
    "        \n",
    "        self.mlp_heads = nn.ModuleList([nn.Sequential(\n",
    "                                        nn.LayerNorm(embed_dim),\n",
    "                                        nn.Linear(embed_dim, embed_dim//2),\n",
    "                                        nn.Dropout(p=mlp_dropout), \n",
    "                                        nn.GELU(),\n",
    "                                        nn.Linear(embed_dim//2, 1)\n",
    "                                        ) for i in range(self.n_roi)])\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x.shape = > [batch, n_electrodes, embed_dim, sequence_length]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        batch = x.shape[0]\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # [batch, sequence_length, n_electrodes, embed_dim]\n",
    "        # spatial transformer.\n",
    "        \n",
    "        x_spatial = x.reshape(batch*self.sequence_length, self.n_electrodes, self.embed_dim)\n",
    "        \n",
    "        roi_tokens = self.roi_tokens.expand(batch*self.sequence_length, -1, -1)\n",
    "        x_spatial = torch.cat((roi_tokens, x_spatial), dim=1)\n",
    "        x_spatial += self.pe_spatial\n",
    "        x_spatial_transformed = self.spatial_transformer(x_spatial)\n",
    "        \n",
    "        roi_tokens = x_spatial_transformed[:, :self.n_roi]\n",
    "        \n",
    "        # OUTPUT: [batch*sequence_length, n_roi, embed_dim]\n",
    "        ## temporal transfromer.\n",
    "        # take only roi tokens \n",
    "        \n",
    "        roi_tokens = roi_tokens.reshape(batch, self.sequence_length, self.n_roi, self.embed_dim)\n",
    "        roi_tokens = roi_tokens.permute(0, 2, 1, 3)\n",
    "        # [batch, n_roi, sequence_length, embed_dim]\n",
    "        \n",
    "        time_tokens = self.time_tokens.expand(batch, -1, -1, -1)\n",
    "        roi_tokens = torch.cat((time_tokens, roi_tokens), dim=2)  # [batch, n_roi, 1+sequence_length, embed_dim]\n",
    "        roi_tokens = roi_tokens.reshape(batch*self.n_roi, 1 + self.sequence_length, self.embed_dim)\n",
    "        # [batch*n_roi, 1+sequence_length, embed_dim]\n",
    "        \n",
    "        \n",
    "        roi_tokens += self.pe_temporal\n",
    "        roi_tokens = self.temporal_transformer(roi_tokens)\n",
    "        roi_tokens = roi_tokens[:, 0]\n",
    "        roi_tokens = roi_tokens.reshape(batch, self.n_roi, self.embed_dim)\n",
    "        \n",
    "        # prediction \n",
    "        preds = []\n",
    "        for roi in range(self.n_roi):\n",
    "            res = self.mlp_heads[roi](roi_tokens[:, roi])\n",
    "            preds.append(res)\n",
    "        preds= torch.cat(preds, dim=1)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd030a0-cad1-47a9-9678-6367999cb41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69175e-9baa-4645-ae6b-15b445a0fcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "afc2c289-9ca8-408e-9b29-83c8bc47ca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 16, 4096])\n"
     ]
    }
   ],
   "source": [
    "n_electrodes = 64\n",
    "n_features = 16\n",
    "window_size = 4096\n",
    "n_time_points = 32 \n",
    "stride = window_size//n_time_points\n",
    "\n",
    "eeg = torch.ones([2, n_electrodes, n_features, window_size])\n",
    "print(eeg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b764794f-a6a2-408f-8b34-ce15f7708591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size torch.Size([2, 64, 16, 4096])\n",
      "Output size complex torch.Size([2, 64, 32, 32])\n",
      "Output size simple  torch.Size([2, 64, 16, 32])\n",
      "Input size:  torch.Size([2, 64, 32, 32])\n",
      "After reshape:  torch.Size([2, 2048, 32])\n",
      "Prediction size  torch.Size([2, 21])\n",
      "Prediction size  torch.Size([2, 21])\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "      Conv_block-1       [2, 32, 1024]           4,672           4,672\n",
      "      Conv_block-2        [2, 32, 256]           5,184           5,184\n",
      "      Conv_block-3        [2, 32, 128]           3,136           3,136\n",
      "      Conv_block-4         [2, 32, 64]           3,136           3,136\n",
      "      Conv_block-5         [2, 32, 32]           3,136           3,136\n",
      "=======================================================================\n",
      "Total params: 19,264\n",
      "Trainable params: 19,264\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "Input size:  torch.Size([2, 64, 32, 32])\n",
      "After reshape:  torch.Size([2, 2048, 32])\n",
      "----------------------------------------------------------------------------\n",
      "           Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "   TransformerEncoder-1       [2, 2049, 32]          25,408          25,408\n",
      "            LayerNorm-2             [2, 32]              64              64\n",
      "               Linear-3            [2, 128]           4,224           4,224\n",
      "              Dropout-4            [2, 128]               0               0\n",
      "                 GELU-5            [2, 128]               0               0\n",
      "               Linear-6             [2, 21]           2,709           2,709\n",
      "============================================================================\n",
      "Total params: 32,405\n",
      "Trainable params: 32,405\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------\n",
      "           Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "   TransformerEncoder-1        [64, 85, 32]          25,408          25,408\n",
      "   TransformerEncoder-2        [42, 33, 32]          25,408          25,408\n",
      "            LayerNorm-3             [2, 32]              64              64\n",
      "               Linear-4             [2, 16]             528             528\n",
      "              Dropout-5             [2, 16]               0               0\n",
      "                 GELU-6             [2, 16]               0               0\n",
      "               Linear-7              [2, 1]              17              17\n",
      "            LayerNorm-8             [2, 32]              64              64\n",
      "               Linear-9             [2, 16]             528             528\n",
      "             Dropout-10             [2, 16]               0               0\n",
      "                GELU-11             [2, 16]               0               0\n",
      "              Linear-12              [2, 1]              17              17\n",
      "           LayerNorm-13             [2, 32]              64              64\n",
      "              Linear-14             [2, 16]             528             528\n",
      "             Dropout-15             [2, 16]               0               0\n",
      "                GELU-16             [2, 16]               0               0\n",
      "              Linear-17              [2, 1]              17              17\n",
      "           LayerNorm-18             [2, 32]              64              64\n",
      "              Linear-19             [2, 16]             528             528\n",
      "             Dropout-20             [2, 16]               0               0\n",
      "                GELU-21             [2, 16]               0               0\n",
      "              Linear-22              [2, 1]              17              17\n",
      "           LayerNorm-23             [2, 32]              64              64\n",
      "              Linear-24             [2, 16]             528             528\n",
      "             Dropout-25             [2, 16]               0               0\n",
      "                GELU-26             [2, 16]               0               0\n",
      "              Linear-27              [2, 1]              17              17\n",
      "           LayerNorm-28             [2, 32]              64              64\n",
      "              Linear-29             [2, 16]             528             528\n",
      "             Dropout-30             [2, 16]               0               0\n",
      "                GELU-31             [2, 16]               0               0\n",
      "              Linear-32              [2, 1]              17              17\n",
      "           LayerNorm-33             [2, 32]              64              64\n",
      "              Linear-34             [2, 16]             528             528\n",
      "             Dropout-35             [2, 16]               0               0\n",
      "                GELU-36             [2, 16]               0               0\n",
      "              Linear-37              [2, 1]              17              17\n",
      "           LayerNorm-38             [2, 32]              64              64\n",
      "              Linear-39             [2, 16]             528             528\n",
      "             Dropout-40             [2, 16]               0               0\n",
      "                GELU-41             [2, 16]               0               0\n",
      "              Linear-42              [2, 1]              17              17\n",
      "           LayerNorm-43             [2, 32]              64              64\n",
      "              Linear-44             [2, 16]             528             528\n",
      "             Dropout-45             [2, 16]               0               0\n",
      "                GELU-46             [2, 16]               0               0\n",
      "              Linear-47              [2, 1]              17              17\n",
      "           LayerNorm-48             [2, 32]              64              64\n",
      "              Linear-49             [2, 16]             528             528\n",
      "             Dropout-50             [2, 16]               0               0\n",
      "                GELU-51             [2, 16]               0               0\n",
      "              Linear-52              [2, 1]              17              17\n",
      "           LayerNorm-53             [2, 32]              64              64\n",
      "              Linear-54             [2, 16]             528             528\n",
      "             Dropout-55             [2, 16]               0               0\n",
      "                GELU-56             [2, 16]               0               0\n",
      "              Linear-57              [2, 1]              17              17\n",
      "           LayerNorm-58             [2, 32]              64              64\n",
      "              Linear-59             [2, 16]             528             528\n",
      "             Dropout-60             [2, 16]               0               0\n",
      "                GELU-61             [2, 16]               0               0\n",
      "              Linear-62              [2, 1]              17              17\n",
      "           LayerNorm-63             [2, 32]              64              64\n",
      "              Linear-64             [2, 16]             528             528\n",
      "             Dropout-65             [2, 16]               0               0\n",
      "                GELU-66             [2, 16]               0               0\n",
      "              Linear-67              [2, 1]              17              17\n",
      "           LayerNorm-68             [2, 32]              64              64\n",
      "              Linear-69             [2, 16]             528             528\n",
      "             Dropout-70             [2, 16]               0               0\n",
      "                GELU-71             [2, 16]               0               0\n",
      "              Linear-72              [2, 1]              17              17\n",
      "           LayerNorm-73             [2, 32]              64              64\n",
      "              Linear-74             [2, 16]             528             528\n",
      "             Dropout-75             [2, 16]               0               0\n",
      "                GELU-76             [2, 16]               0               0\n",
      "              Linear-77              [2, 1]              17              17\n",
      "           LayerNorm-78             [2, 32]              64              64\n",
      "              Linear-79             [2, 16]             528             528\n",
      "             Dropout-80             [2, 16]               0               0\n",
      "                GELU-81             [2, 16]               0               0\n",
      "              Linear-82              [2, 1]              17              17\n",
      "           LayerNorm-83             [2, 32]              64              64\n",
      "              Linear-84             [2, 16]             528             528\n",
      "             Dropout-85             [2, 16]               0               0\n",
      "                GELU-86             [2, 16]               0               0\n",
      "              Linear-87              [2, 1]              17              17\n",
      "           LayerNorm-88             [2, 32]              64              64\n",
      "              Linear-89             [2, 16]             528             528\n",
      "             Dropout-90             [2, 16]               0               0\n",
      "                GELU-91             [2, 16]               0               0\n",
      "              Linear-92              [2, 1]              17              17\n",
      "           LayerNorm-93             [2, 32]              64              64\n",
      "              Linear-94             [2, 16]             528             528\n",
      "             Dropout-95             [2, 16]               0               0\n",
      "                GELU-96             [2, 16]               0               0\n",
      "              Linear-97              [2, 1]              17              17\n",
      "           LayerNorm-98             [2, 32]              64              64\n",
      "              Linear-99             [2, 16]             528             528\n",
      "            Dropout-100             [2, 16]               0               0\n",
      "               GELU-101             [2, 16]               0               0\n",
      "             Linear-102              [2, 1]              17              17\n",
      "          LayerNorm-103             [2, 32]              64              64\n",
      "             Linear-104             [2, 16]             528             528\n",
      "            Dropout-105             [2, 16]               0               0\n",
      "               GELU-106             [2, 16]               0               0\n",
      "             Linear-107              [2, 1]              17              17\n",
      "============================================================================\n",
      "Total params: 63,605\n",
      "Trainable params: 63,605\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "config_feature_extractor = dict(n_inp_features=n_features,\n",
    "                                 channels = [32, 32, 32, 32, 32], \n",
    "                                 kernel_sizes=[9, 5, 3, 3, 3],\n",
    "                                 strides=[4, 4, 2, 2, 2], \n",
    "                                 dilations = [1, 1, 1, 1, 1])\n",
    "\n",
    "\n",
    "config_vanilla_transformer = dict(n_electrodes = n_electrodes,\n",
    "                                 sequence_length = n_time_points,\n",
    "                                 embed_dim=32,\n",
    "                                 n_roi=21,\n",
    "                                 num_heads=4,\n",
    "                                 mlp_ratio=4,\n",
    "                                 num_layers=2,\n",
    "                                 attn_dropout=0.1,                \n",
    "                                 mlp_dropout=0.5)\n",
    "\n",
    "config_factorized_transformer = dict(n_electrodes = n_electrodes,\n",
    "                                 sequence_length = n_time_points,\n",
    "                                 embed_dim=32,\n",
    "                                 n_roi=21,\n",
    "                                 num_heads=4,\n",
    "                                 mlp_ratio=4,\n",
    "                                 num_layers_spatial=2,\n",
    "                                 num_layers_temporal=2,\n",
    "                                 attn_dropout=0.1,                \n",
    "                                 mlp_dropout=0.5)\n",
    "\n",
    "model_wav2net = wav2vec_conv(**config_feature_extractor)\n",
    "vit = Vanilla_transformer(**config_vanilla_transformer)\n",
    "factorizes_vit = Factorized_transformer(**config_factorized_transformer)\n",
    "\n",
    "\n",
    "eeg_simple_features = eeg[..., ::stride]\n",
    "eeg_complex_features = torch.stack([model_wav2net(eeg[:, i]) for i in range(n_electrodes)], 1)\n",
    "print('Input size', eeg.shape)\n",
    "\n",
    "print('Output size complex', eeg_complex_features.shape)\n",
    "print('Output size simple ', eeg_simple_features.shape)\n",
    "\n",
    "y_hat = vit(eeg_complex_features)\n",
    "y_hat_2 = factorizes_vit(eeg_complex_features)\n",
    "\n",
    "\n",
    "print('Prediction size ', y_hat.shape)\n",
    "print('Prediction size ', y_hat_2.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(summary(model_wav2net, eeg[:, 0], show_input=False))\n",
    "print(summary(vit, eeg_complex_features, show_input=False))\n",
    "print(summary(factorizes_vit, eeg_complex_features, show_input=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f77cb6-9391-4777-8948-85400ddecba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f417d-061d-47f5-bca2-848a7f5f6959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2fa89e66-100b-4aee-af2d-ef8427e61ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_spatial = nn.Parameter(torch.zeros(1, n_electrodes, 1, 128), requires_grad=True)\n",
    "pe_temporal = nn.Parameter(torch.zeros(1, 1, 64 + 1, 128), requires_grad=True)\n",
    "\n",
    "pe_merged = pe_spatial + pe_temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "45b545d8-ccfa-4a8a-ad6a-0536e9d315ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 65, 128])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2069bdfa-6112-4359-b406-566e85323605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832a28e-6dca-4064-a543-9bfa3241646f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_torch",
   "language": "python",
   "name": "myenv_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
