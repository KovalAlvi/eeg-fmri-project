{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6b2c92-ed9d-4743-8d8a-a780b42d24af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interpretable Transformer-based architecture.\n",
    "\n",
    "Pretrain via MAE: -> https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/mae.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cda5312-c3b4-43df-a03c-1f7ed4ed8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.9/site-packages/nilearn/datasets/__init__.py:93: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "\n",
    "import sklearn\n",
    "import mne\n",
    "import wandb\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "\n",
    "\n",
    "from nilearn import image\n",
    "from nilearn import plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from nilearn import datasets, image, masking\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "\n",
    "\n",
    "# animation part\n",
    "from IPython.display import HTML\n",
    "# from celluloid import Camera   # it is convinient method to animate\n",
    "from matplotlib import animation, rc\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "\n",
    "## torch libraries \n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from pytorch_model_summary import summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ec777-1abc-48ea-9a98-a4d7efc93528",
   "metadata": {},
   "source": [
    "# Feature extractor.\n",
    "- ConvNext and wav2vec inspired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97377e15-d4f7-43c6-8bd9-b9a8d571ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Input is [batch, emb, time]\n",
    "    simple conv block from wav2vec 2.0 \n",
    "        - conv\n",
    "        - layer norm by embedding axis\n",
    "        - activation\n",
    "    To do: \n",
    "        add res blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1, dilation=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        \n",
    "        # use it instead stride. \n",
    "        self.downsample = nn.AvgPool1d(kernel_size=stride, stride=stride)\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, \n",
    "                                kernel_size=kernel_size, \n",
    "                                bias=False, \n",
    "                                padding='same')\n",
    "        \n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        - downsample \n",
    "        - conv \n",
    "        - norm \n",
    "        - activation\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.downsample(x)\n",
    "        \n",
    "        x = self.conv1d(x)\n",
    "        \n",
    "        # norm by last axis.\n",
    "        x = torch.transpose(x, -2, -1) \n",
    "        x = self.norm(x)\n",
    "        x = torch.transpose(x, -2, -1) \n",
    "        \n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class wav2vec_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Extract some features from one time serias as raw speech.\n",
    "    To do make it possibl;e to work with raw EEG electrode recording. \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_inp_features=30,\n",
    "                 channels = [32, 32, 32], \n",
    "                 kernel_sizes=[3, 3, 3],\n",
    "                 strides=[2, 2, 2], \n",
    "                 dilations = [1, 1, 1]):\n",
    "        \n",
    "        super(wav2vec_conv, self).__init__()\n",
    "        \n",
    "        # freqs-electrodes  reducing to channels[0].\n",
    "        # add additional layer\n",
    "        channels = [n_inp_features] + channels\n",
    "        \n",
    "        self.model_depth = len(channels)-1\n",
    "        # create downsample blcoks in Sequentional manner.\n",
    "        self.downsample_blocks = nn.ModuleList([Conv_block(channels[i], \n",
    "                                                        channels[i+1], \n",
    "                                                        kernel_sizes[i],\n",
    "                                                        stride=strides[i], \n",
    "                                                        dilation=dilations[i]) for i in range(self.model_depth)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. Encode  \n",
    "        \"\"\"\n",
    "        batch, n_freq, time = x.shape\n",
    "    \n",
    "        # encode information\n",
    "        for block  in self.downsample_blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23e0e4de-bf7f-4599-97df-f117a252103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec_aggregate(nn.Module):\n",
    "    \"\"\"\n",
    "    Inpyt should be. \n",
    "    batch, n_ch, n_freq, time = x.shape\n",
    "    \n",
    "    model_embedding - should be work with [batch, n_freqs, time]\n",
    "    \n",
    "    Return [batch, n_ch, emb, time//stride] \n",
    "    \"\"\"\n",
    "    def __init__(self, model_embedding):\n",
    "        \n",
    "        super(Wav2Vec_aggregate, self).__init__()\n",
    "        \n",
    "        # create downsample blcoks in Sequentional manner.\n",
    "        self.embedding = model_embedding\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. Apply for each channnels.  \n",
    "        \"\"\"\n",
    "        batch, n_ch, n_freq, time = x.shape\n",
    "        emb_list = [self.embedding(x[:, ch]) for ch in range(n_ch)]\n",
    "        emb_list = torch.stack(emb_list, dim = 1)\n",
    "        return emb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b534a-e43b-4979-8c06-234421d0d195",
   "metadata": {},
   "source": [
    "# Extract features from EEG \n",
    "- Raw signal \n",
    "    - apply wav2net encoder for each electrode separately \n",
    "- Wavelet features \n",
    "    - Apply wav2net separately for better features extraction \n",
    "    - Simple downsampling temporal part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58a97bb9-c89f-4544-b84d-a930a46897d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57f1fd86-afb2-46cf-a243-9d51e23fd043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d795ff0e-da97-494e-a5eb-39ce70f105fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla transformer aggregate all \n",
    "    [batch, n_electrodes, embed_dim, time]\n",
    "    \n",
    "    Input of transformer is -> [batch, sequence_length, embed_dim]\n",
    "    \n",
    "    Return \n",
    "        [batch, 21]\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_electrodes = 64,\n",
    "                 sequence_length = 128,\n",
    "                 embed_dim=256,\n",
    "                 n_roi=21,\n",
    "                 num_heads=4,\n",
    "                 mlp_ratio=2,\n",
    "                 attn_dropout=0.1,\n",
    "                 num_layers=1,                \n",
    "                 mlp_dropout=0.1,\n",
    "                ):\n",
    "        super(Vanilla_transformer, self).__init__()\n",
    "        self.n_electrodes = n_electrodes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.class_embed = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        \n",
    "        # just add vector \n",
    "        self.pe = nn.Parameter(torch.zeros(1, n_electrodes*sequence_length + 1, embed_dim), requires_grad=True)\n",
    "        \n",
    "        # less parameters. factorises pe.\n",
    "\n",
    "        self.pe_spatial = nn.Parameter(torch.zeros(1, n_electrodes, 1, embed_dim), requires_grad=True)\n",
    "        self.pe_temporal = nn.Parameter(torch.zeros(1, 1, sequence_length , embed_dim), requires_grad=True)\n",
    "        self.pe_cls =  nn.Parameter(torch.zeros(1, 1, self.embed_dim),  requires_grad=True)\n",
    "\n",
    "\n",
    "        \n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                            nhead=num_heads, \n",
    "                                                            dim_feedforward=embed_dim*mlp_ratio, \n",
    "                                                            dropout=attn_dropout, \n",
    "                                                            activation='relu', \n",
    "                                                            batch_first=True)\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        # take our vector and make 21 prediction. \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*mlp_ratio),\n",
    "            nn.Dropout(p=mlp_dropout), \n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim*mlp_ratio, n_roi))\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x.shape = > [batch, n_electrodes, embed_dim, time]\n",
    "        \n",
    "        \"\"\"\n",
    "        print('Input size: ', x.shape)\n",
    "        batch = x.shape[0]\n",
    "        x = x.transpose(2, 3)\n",
    "        # print('After reshape: ', x.shape)\n",
    "        # print(self.sequence_length , self.embed_dim, self.embed_dim)\n",
    "        x = x.reshape(batch, self.sequence_length * self.n_electrodes, self.embed_dim)\n",
    "        print('After reshape: ', x.shape)\n",
    "        \n",
    "        # repeating like batch size and add seg lenght. \n",
    "        class_token = self.class_embed.expand(batch, -1, -1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "        \n",
    "        # add positional embeddings [batch, sequence_length, embed_dim]\n",
    "        self.merge_pos_encoding()\n",
    "        \n",
    "        x += self.pe_merged\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # take first vector -> normalize -> mlp into \n",
    "        x_cls = x[:, 0]\n",
    "        x_cls = self.mlp_head(self.norm(x_cls))\n",
    "        return x_cls\n",
    "    \n",
    "    def merge_pos_encoding(self):\n",
    "        \"\"\"\n",
    "        for calcualation.\n",
    "        \"\"\"\n",
    "        self.pe_merged = self.pe_spatial + self.pe_temporal\n",
    "        self.pe_merged = self.pe_merged.reshape(1, self.n_electrodes*self.sequence_length, self.embed_dim)\n",
    "        self.pe_merged = torch.cat([self.pe_cls, self.pe_merged], dim= 1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e13acc1a-ab02-4da0-9d08-ef04f6b3103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Factorized_transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla transformer aggregate all \n",
    "    [batch, n_electrodes, embed_dim, time]\n",
    "    \n",
    "    Input of transformer is -> [batch, sequence_length, embed_dim]\n",
    "    \n",
    "    \n",
    "    \n",
    "    Spatial transformer ( electrode aggregation ):\n",
    "        [batch, n_electrodes, embed_dim, time] -> [batch, n_roi, embed_dim, time]\n",
    "    Temporal transformer: (time aggregation) \n",
    "        [batch, n_roi, embed_dim, time] -> [batch, n_roi, embed_dim]\n",
    "    Prediction\n",
    "         [batch, n_roi, embed_dim] -> [batch, n_roi]\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    Return \n",
    "        [batch, 21]\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_electrodes = 64,\n",
    "                 sequence_length = 128,\n",
    "                 embed_dim=256,\n",
    "                 n_roi=21,\n",
    "                 num_heads=4,\n",
    "                 mlp_ratio=2,\n",
    "                 attn_dropout=0.1,\n",
    "                 num_layers_spatial=1,\n",
    "                 num_layers_temporal=1,\n",
    "                 mlp_dropout=0.1,\n",
    "                ):\n",
    "        super(Factorized_transformer, self).__init__()\n",
    "        self.n_electrodes = n_electrodes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_roi = n_roi\n",
    "        \n",
    "                \n",
    "        self.roi_tokens = nn.Parameter(torch.zeros(1, n_roi, embed_dim), requires_grad=True)\n",
    "        self.time_tokens = nn.Parameter(torch.zeros(1, n_roi, 1, embed_dim), requires_grad=True) # use different reg tokent for each roi.\n",
    "   \n",
    "\n",
    "        self.pe_spatial = nn.Parameter(torch.zeros(1, n_roi + n_electrodes, embed_dim), requires_grad=True)\n",
    "        self.pe_temporal = nn.Parameter(torch.zeros(1, 1 + sequence_length, embed_dim), requires_grad=True)\n",
    "        \n",
    "        \n",
    "        spatial_layer = nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                                nhead=num_heads, \n",
    "                                                                dim_feedforward=embed_dim*mlp_ratio, \n",
    "                                                                dropout=attn_dropout, \n",
    "                                                                activation='relu', \n",
    "                                                                batch_first=True)\n",
    "        \n",
    "        temporal_layer = nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                            nhead=num_heads, \n",
    "                                                            dim_feedforward=embed_dim*mlp_ratio, \n",
    "                                                            dropout=attn_dropout, \n",
    "                                                            activation='relu', \n",
    "                                                            batch_first=True)\n",
    "        \n",
    "        self.spatial_transformer = nn.TransformerEncoder(spatial_layer, num_layers=num_layers_spatial)\n",
    "        self.temporal_transformer = nn.TransformerEncoder(temporal_layer, num_layers=num_layers_temporal)\n",
    "\n",
    "\n",
    "\n",
    "#         self.mlp_heads = nn.ModuleList([nn.Sequential(\n",
    "#                                         nn.LayerNorm(embed_dim),\n",
    "#                                         nn.Linear(embed_dim, embed_dim*mlp_ratio),\n",
    "#                                         nn.Dropout(p=mlp_dropout), \n",
    "#                                         nn.GELU(),\n",
    "#                                         nn.Linear(embed_dim*mlp_ratio, 1)\n",
    "#                                         ) for i in range(self.n_roi)])\n",
    "        \n",
    "        self.mlp_heads = nn.ModuleList([nn.Sequential(\n",
    "                                        nn.LayerNorm(embed_dim),\n",
    "                                        nn.Linear(embed_dim, embed_dim//2),\n",
    "                                        nn.Dropout(p=mlp_dropout), \n",
    "                                        nn.GELU(),\n",
    "                                        nn.Linear(embed_dim//2, 1)\n",
    "                                        ) for i in range(self.n_roi)])\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x.shape = > [batch, n_electrodes, embed_dim, sequence_length]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        batch = x.shape[0]\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # [batch, sequence_length, n_electrodes, embed_dim]\n",
    "        # spatial transformer.\n",
    "        \n",
    "        x_spatial = x.reshape(batch*self.sequence_length, self.n_electrodes, self.embed_dim)\n",
    "        \n",
    "        roi_tokens = self.roi_tokens.expand(batch*self.sequence_length, -1, -1)\n",
    "        x_spatial = torch.cat((roi_tokens, x_spatial), dim=1)\n",
    "        x_spatial += self.pe_spatial\n",
    "        x_spatial_transformed = self.spatial_transformer(x_spatial)\n",
    "        \n",
    "        roi_tokens = x_spatial_transformed[:, :self.n_roi]\n",
    "        \n",
    "        # OUTPUT: [batch*sequence_length, n_roi, embed_dim]\n",
    "        ## temporal transfromer.\n",
    "        # take only roi tokens \n",
    "        \n",
    "        roi_tokens = roi_tokens.reshape(batch, self.sequence_length, self.n_roi, self.embed_dim)\n",
    "        roi_tokens = roi_tokens.permute(0, 2, 1, 3)\n",
    "        # [batch, n_roi, sequence_length, embed_dim]\n",
    "        \n",
    "        time_tokens = self.time_tokens.expand(batch, -1, -1, -1)\n",
    "        roi_tokens = torch.cat((time_tokens, roi_tokens), dim=2)  # [batch, n_roi, 1+sequence_length, embed_dim]\n",
    "        roi_tokens = roi_tokens.reshape(batch*self.n_roi, 1 + self.sequence_length, self.embed_dim)\n",
    "        # [batch*n_roi, 1+sequence_length, embed_dim]\n",
    "        \n",
    "        \n",
    "        roi_tokens += self.pe_temporal\n",
    "        roi_tokens = self.temporal_transformer(roi_tokens)\n",
    "        roi_tokens = roi_tokens[:, 0]\n",
    "        roi_tokens = roi_tokens.reshape(batch, self.n_roi, self.embed_dim)\n",
    "        \n",
    "        # prediction \n",
    "        preds = []\n",
    "        for roi in range(self.n_roi):\n",
    "            res = self.mlp_heads[roi](roi_tokens[:, roi])\n",
    "            preds.append(res)\n",
    "        preds= torch.cat(preds, dim=1)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd030a0-cad1-47a9-9678-6367999cb41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3e69175e-9baa-4645-ae6b-15b445a0fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strided_func(stride):\n",
    "    \"\"\"\n",
    "    stride parameters pooling \n",
    "    pool by last axis .\n",
    "    \"\"\"\n",
    "    def get_strided_input(x):\n",
    "        return x[..., ::stride]\n",
    "    return get_strided_input\n",
    "\n",
    "\n",
    "class Super_model(nn.Module):\n",
    "    \"\"\"\n",
    "    Inpyt should be. \n",
    "    batch, n_ch, n_freq, time = x.shape\n",
    "    \n",
    "    \n",
    "    Return [batch, n_ch_out, emb, time//stride] \n",
    "    \"\"\"\n",
    "    def __init__(self, feature_extractor, transformer):\n",
    "        \n",
    "        super(Super_model, self).__init__()\n",
    "        \n",
    "        # create downsample blcoks in Sequentional manner.\n",
    "        \n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. Apply for each channnels.  \n",
    "        \"\"\"\n",
    "        batch, n_ch, n_freq, time = x.shape\n",
    "        \n",
    "        x_features = self.feature_extractor(x)\n",
    "        print('Size', x_features.shape)\n",
    "        x_out = self.transformer(x_features)\n",
    "        \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "afc2c289-9ca8-408e-9b29-83c8bc47ca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 16, 4096])\n"
     ]
    }
   ],
   "source": [
    "n_electrodes = 64\n",
    "n_features = 16\n",
    "window_size = 4096\n",
    "n_time_points = 32 \n",
    "stride = window_size//n_time_points\n",
    "\n",
    "eeg = torch.ones([2, n_electrodes, n_features, window_size])\n",
    "print(eeg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b764794f-a6a2-408f-8b34-ce15f7708591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 16, 4096])\n",
      "Size torch.Size([2, 64, 16, 32])\n",
      "Input size:  torch.Size([2, 64, 16, 32])\n",
      "After reshape:  torch.Size([2, 2048, 16])\n",
      "Size torch.Size([2, 64, 32, 32])\n",
      "Input size torch.Size([2, 64, 16, 4096])\n",
      "Prediction size  torch.Size([2, 21])\n",
      "Prediction size 2 torch.Size([2, 21])\n",
      "Size torch.Size([2, 64, 16, 32])\n",
      "Input size:  torch.Size([2, 64, 16, 32])\n",
      "After reshape:  torch.Size([2, 2048, 16])\n",
      "-----------------------------------------------------------------------------\n",
      "            Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "   Vanilla_transformer-1             [2, 21]          43,397          43,397\n",
      "=============================================================================\n",
      "Total params: 43,397\n",
      "Trainable params: 43,397\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------\n",
      "Size torch.Size([2, 64, 32, 32])\n",
      "--------------------------------------------------------------------------------\n",
      "               Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "================================================================================\n",
      "        Wav2Vec_aggregate-1     [2, 64, 32, 32]          19,264          19,264\n",
      "   Factorized_transformer-2             [2, 21]          68,725          68,725\n",
      "================================================================================\n",
      "Total params: 87,989\n",
      "Trainable params: 87,989\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_electrodes = 64\n",
    "n_features = 16\n",
    "window_size = 4096\n",
    "n_time_points = 32 \n",
    "stride = window_size//n_time_points\n",
    "\n",
    "eeg = torch.ones([2, n_electrodes, n_features, window_size])\n",
    "print(eeg.shape)\n",
    "\n",
    "\n",
    "config_feature_extractor = dict(n_inp_features=n_features,\n",
    "                                 channels = [32, 32, 32, 32, 32], \n",
    "                                 kernel_sizes=[9, 5, 3, 3, 3],\n",
    "                                 strides=[4, 4, 2, 2, 2], \n",
    "                                 dilations = [1, 1, 1, 1, 1])\n",
    "\n",
    "\n",
    "config_vanilla_transformer = dict(n_electrodes = n_electrodes,\n",
    "                                 sequence_length = n_time_points,\n",
    "                                 embed_dim=n_features,\n",
    "                                 n_roi=21,\n",
    "                                 num_heads=4,\n",
    "                                 mlp_ratio=4,\n",
    "                                 num_layers=2,\n",
    "                                 attn_dropout=0.1,                \n",
    "                                 mlp_dropout=0.5)\n",
    "\n",
    "config_factorized_transformer = dict(n_electrodes = n_electrodes,\n",
    "                                 sequence_length = n_time_points,\n",
    "                                 embed_dim=32,\n",
    "                                 n_roi=21,\n",
    "                                 num_heads=4,\n",
    "                                 mlp_ratio=4,\n",
    "                                 num_layers_spatial=2,\n",
    "                                 num_layers_temporal=2,\n",
    "                                 attn_dropout=0.1,                \n",
    "                                 mlp_dropout=0.5)\n",
    "\n",
    "conv_wav2net = wav2vec_conv(**config_feature_extractor)\n",
    "model_wav2net = Wav2Vec_aggregate(conv_wav2net)\n",
    "\n",
    "\n",
    "\n",
    "vit = Vanilla_transformer(**config_vanilla_transformer)\n",
    "factorized_vit = Factorized_transformer(**config_factorized_transformer)\n",
    "\n",
    "\n",
    "model_v1 = Super_model(feature_extractor = get_strided_func(128), \n",
    "                       transformer = vit)\n",
    "\n",
    "\n",
    "model_v2 = Super_model(feature_extractor = model_wav2net, \n",
    "                       transformer = factorized_vit)\n",
    "\n",
    "\n",
    "\n",
    "y_hat = model_v1(eeg) \n",
    "y_hat_2 = model_v2(eeg) \n",
    "\n",
    "print('Input size', eeg.shape)\n",
    "print('Prediction size ', y_hat.shape)\n",
    "print('Prediction size 2', y_hat_2.shape)\n",
    "\n",
    "\n",
    "# print('Output size complex', eeg_complex_features.shape)\n",
    "# print('Output size simple ', eeg_simple_features.shape)\n",
    "\n",
    "\n",
    "\n",
    "# y_hat = vit(eeg_complex_features)\n",
    "# y_hat_2 = factorizes_vit(eeg_complex_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(summary(model_v1, eeg, show_input=False))\n",
    "print(summary(model_v2, eeg, show_input=False))\n",
    "# print(summary(factorizes_vit, eeg_complex_features, show_input=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "71a20c61-9389-4bf4-9e65-f2715bc348f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size torch.Size([2, 64, 16, 32])\n",
      "Input size:  torch.Size([2, 64, 16, 32])\n",
      "After reshape:  torch.Size([2, 2048, 16])\n",
      "-----------------------------------------------------------------------------\n",
      "            Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "   Vanilla_transformer-1             [2, 21]          43,397          43,397\n",
      "=============================================================================\n",
      "Total params: 43,397\n",
      "Trainable params: 43,397\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_v1 = model_v1.cuda()\n",
    "print(summary(model_v1, eeg.cuda(), show_input=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "18f77cb6-9391-4777-8948-85400ddecba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 16, 32])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = get_strided_func(128)\n",
    "func(eeg).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b4649-bbd2-4429-b984-7c8704d41d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "316f62c3-498e-42ab-b16d-811aca369d8e",
   "metadata": {},
   "source": [
    "# Following experiments \n",
    "\n",
    "1. Vanilla Transformer v1/v2. \n",
    "    - strided input / Extracted features.\n",
    "    - vanilla transformer.\n",
    "    \n",
    "\n",
    "2. Factorized Transformer v1/v2. \n",
    "    - strided input / Extracted features.\n",
    "    - Factorized transformer.\n",
    "    \n",
    "    \n",
    "3. SSL transforemr.\n",
    "    - strided input + Masked tokens \n",
    "    - Vanilla transformer learn to predict MASK.\n",
    "    - EEG pretrain \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "## Diffusion models \n",
    "- Create universal 1D UNet model( simple )\n",
    "1. Generate fMRI samples from noise Normal  \n",
    "2. Generate fMRI sample from noise based on EEG data. \n",
    "3. Profit \n",
    "\n",
    "\n",
    "\n",
    "## Normalizing flows\n",
    "NF - change of variables. Transform function we parametrize with NN. \n",
    "So we can build two different NF: for EEG and for fMRI. \n",
    "\n",
    "\n",
    "EEG --> Z --> fMRI   \n",
    "EEG <-- Z <-- fMRI \n",
    "\n",
    "1. Learn generate fmRI. \n",
    "2. Learn generate EEG.\n",
    "3. Get Z for each EEG and for each fMRI. \n",
    "4. Learn connection between different Z. \n",
    "   \n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07825a-7792-4e7a-a8df-875a678dadfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f417d-061d-47f5-bca2-848a7f5f6959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_torch",
   "language": "python",
   "name": "myenv_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
