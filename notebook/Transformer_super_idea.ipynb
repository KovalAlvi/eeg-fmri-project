{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cda5312-c3b4-43df-a03c-1f7ed4ed8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.9/site-packages/nilearn/datasets/__init__.py:93: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "\n",
    "import sklearn\n",
    "import mne\n",
    "import wandb\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "\n",
    "\n",
    "from nilearn import image\n",
    "from nilearn import plotting\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from nilearn import datasets, image, masking\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "\n",
    "\n",
    "# animation part\n",
    "from IPython.display import HTML\n",
    "# from celluloid import Camera   # it is convinient method to animate\n",
    "from matplotlib import animation, rc\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "\n",
    "## torch libraries \n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from pytorch_model_summary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe56de9d-e243-44d9-ad57-ed8880cc24a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c00c233-69b8-4058-ad27-57db2716892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pytorch_model_summary import summary\n",
    "import numpy as np\n",
    "\n",
    "# **Change on 28 November**\n",
    "# - Remove bias in all convolution \n",
    "# - Use GELU instead ReLu\n",
    "# - Change batch on instance  normalization \n",
    "#     - It compute statisctics croos all time points for some channels. -> channels have same distribution.\n",
    "#     - On inference we apply statisctics from training.\n",
    "#     - Might be issue when we inferecen for all timeserais lenght.- Add more filters per channel in SepConv \n",
    "# - dilation the same.\n",
    "\n",
    "class SepConv1D(nn.Module):\n",
    "    def __init__(self, nin, kernels_per_layer, nout, kernel_size=3, dilation=1):\n",
    "        super(SepConv1D, self).__init__()\n",
    "        self.depthwise = nn.Conv1d(nin, nin * kernels_per_layer, \n",
    "                                   kernel_size=kernel_size, \n",
    "                                   padding='same', \n",
    "                                   groups=nin, \n",
    "                                   dilation=dilation, \n",
    "                                   bias = False)\n",
    "        self.pointwise = nn.Conv1d(nin * kernels_per_layer, nout, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Input is [batch, emb, time]\n",
    "    simple conv block from wav2vec 2.0 \n",
    "        - conv\n",
    "        - layer norm by embedding axis\n",
    "        - activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1, dilation=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        \n",
    "        # use it instead stride. \n",
    "        self.downsample = nn.AveragePool1d(kernel_size=stride, stride=stride)\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, bias=False)\n",
    "        self.norm = nn.LayerNorm1d(out_channels) \n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.downsample(x)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    \n",
    "class UpsampleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, scale=2):\n",
    "        super(UpsampleConvBlock, self).__init__()\n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=scale, mode='linear')\n",
    "        self.conv_block = Block1D(in_channels, out_channels, kernel_size)\n",
    "            \n",
    "    def forward(self, x_small):\n",
    "        \n",
    "        x_upsample = self.upsample(x_small)\n",
    "        x = self.conv_block(x_upsample)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "class AutoEncoder1D(nn.Module):\n",
    "    \"\"\"\n",
    "    This is implementaiotn of AutoEncoder1D model for time serias regression\n",
    "    Try to adopt fully convolutional network for fMRI decoding \n",
    "    1. Denoise stage - Use res blocks for it \n",
    "    2. Encode information using 1D convolution with big stride(now use max pool)\n",
    "    3. Decode information using cat pass from previous levels.\n",
    "    4. Map to only 21 ROI\n",
    "    \n",
    "    hp_autoencoder = dict(  n_res_block = 0, \n",
    "                            channels = [32, 32, 64, 64], \n",
    "                            kernel_sizes = [15, 11, 5, 5],\n",
    "                            strides= [8, 8, 4],\n",
    "                            n_channels_out = 21,\n",
    "                            n_channels_inp = 900)\n",
    "\n",
    "\n",
    "    model = AutoEncoder1D(**hp_autoencoder)\n",
    "    print(summary(model, torch.zeros(4, 30, 30 , 1024), show_input=False))\n",
    "\n",
    "    \n",
    "    Time lenght of input and output the same. \n",
    "    \"\"\"\n",
    "    def __init__(self,n_electrodes=30,\n",
    "                 n_freqs = 16,\n",
    "                 n_channels_out=21,\n",
    "                 n_res_block=1,\n",
    "                 channels = [8, 16, 32], \n",
    "                 kernel_sizes=[3, 3, 3],\n",
    "                 strides=[4, 4, 4]):\n",
    "        \n",
    "        super(AutoEncoder1D, self).__init__()\n",
    "        \n",
    "        self.n_res_block = n_res_block\n",
    "        self.model_depth = len(channels)-1\n",
    "\n",
    "        self.n_electrodes = n_electrodes\n",
    "        self.n_freqs = n_freqs\n",
    "        \n",
    "        ## factorized features reducing\n",
    "        # electrodes spatial reducing\n",
    "        self.spatial_reduce_2d = nn.Conv2d(self.n_electrodes, 16, kernel_size=1)\n",
    "        \n",
    "        # freqs-electrodes  reducing to channels[0].\n",
    "        self.spatial_reduce = nn.Conv1d(16*self.n_freqs, channels[0], kernel_size=1)\n",
    "        \n",
    "        \n",
    "        # create downsample blcoks in Sequentional manner.\n",
    "        self.downsample_blocks = nn.ModuleList([Block1D(channels[i], \n",
    "                                                        channels[i+1], \n",
    "                                                        kernel_sizes[i],\n",
    "                                                        stride=strides[i], \n",
    "                                                        dilation=2) for i in range(self.model_depth)])\n",
    "        \n",
    "        \n",
    "        # mapping in latent space\n",
    "        self.mapping = Block1D(channels[-1], channels[-1],\n",
    "                               kernel_sizes[-1], \n",
    "                               stride=1,\n",
    "                               dilation=1)\n",
    "        \n",
    "        self.conv1x1_one = nn.Conv1d(channels[-1], n_channels_out, kernel_size=1, padding='same')\n",
    "\n",
    "        \n",
    "        # upscale one time to. Inverse Interpolation. \n",
    "        scale_one = 2**int(np.sum(np.log2(strides)))\n",
    "        self.upsample_one_time = nn.Upsample(scale_factor=scale_one, mode='linear')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. Denoise \n",
    "        2. Encode  \n",
    "        3. Mapping \n",
    "        4. Decode \n",
    "        \"\"\"\n",
    "        batch, elec, n_freq, time = x.shape\n",
    "        \n",
    "        # x = x.reshape(batch, self.n_electrodes, -1, time)\n",
    "        x = self.spatial_reduce_2d(x)\n",
    "        \n",
    "        x = x.reshape(batch, -1, time)\n",
    "        x = self.spatial_reduce(x)\n",
    "        \n",
    "        # encode information\n",
    "        for i in range(self.model_depth):\n",
    "            x = self.downsample_blocks[i](x)            \n",
    "        \n",
    "        # make mapping \n",
    "        x = self.mapping(x)\n",
    "        x = self.conv1x1_one(x)\n",
    "        \n",
    "        # inverse interpolation\n",
    "        x = self.upsample_one_time(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27500c3c-1031-4bdc-8502-64c658d23ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97377e15-d4f7-43c6-8bd9-b9a8d571ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Input is [batch, emb, time]\n",
    "    simple conv block from wav2vec 2.0 \n",
    "        - conv\n",
    "        - layer norm by embedding axis\n",
    "        - activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1, dilation=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        \n",
    "        # use it instead stride. \n",
    "        self.downsample = nn.AvgPool1d(kernel_size=stride, stride=stride)\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, bias=False, \n",
    "                                padding='same')\n",
    "        \n",
    "        self.norm = nn.LayerNorm(out_channels) \n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.downsample(x)\n",
    "        x = self.conv1d(x)\n",
    "        \n",
    "        # norm by last axis.\n",
    "        x = torch.transpose(x, -2, -1) \n",
    "        x = self.norm(x)\n",
    "        x = torch.transpose(x, -2, -1) \n",
    "        \n",
    "        \n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Wav2vec_Conv(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_inp_features=30,\n",
    "                 channels = [8, 16, 32], \n",
    "                 kernel_sizes=[3, 3, 3],\n",
    "                 strides=[4, 4, 4], \n",
    "                 dilations = [1, 1, 1]):\n",
    "        \n",
    "        super(Conv_Wav2vec, self).__init__()\n",
    "        \n",
    "        # freqs-electrodes  reducing to channels[0].\n",
    "        channels = [n_inp_features] + channels\n",
    "        \n",
    "        self.model_depth = len(channels)-1\n",
    "        # create downsample blcoks in Sequentional manner.\n",
    "        self.downsample_blocks = nn.ModuleList([Conv_block(channels[i], \n",
    "                                                        channels[i+1], \n",
    "                                                        kernel_sizes[i],\n",
    "                                                        stride=strides[i], \n",
    "                                                        dilation=dilations[i]) for i in range(self.model_depth)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. Encode  \n",
    "        \"\"\"\n",
    "        batch, n_freq, time = x.shape\n",
    "    \n",
    "        # encode information\n",
    "        for block  in self.downsample_blocks:\n",
    "            x = block(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0e4de-bf7f-4599-97df-f117a252103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec_aggregate(nn.Module):\n",
    "    \"\"\"\n",
    "    Inpy should be. \n",
    "    batch, n_ch, n_freq, time = x.shape\n",
    "    \n",
    "    model_embedding - should be work with [batch, n_freqs, time]\n",
    "    \n",
    "    Return [batch, n_ch, emb, time//stride] \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_embedding,\n",
    "                 ):\n",
    "        \n",
    "        super(Wav2Vec_aggregate, self).__init__()\n",
    "        \n",
    "        # freqs-electrodes  reducing to channels[0].\n",
    "        channels = [n_inp_features] + channels\n",
    "        \n",
    "        # create downsample blcoks in Sequentional manner.\n",
    "        self.embedding = model_embedding\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1. Encode  \n",
    "        \"\"\"\n",
    "        batch, n_ch, n_freq, time = x.shape\n",
    "        \n",
    "        emb_list  = []\n",
    "        for ch in range(n_ch):\n",
    "            x_ch = self.embedding(x[:, ch])\n",
    "            \n",
    "            emb_list.append(x_emb)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a622b2b-8bdb-466e-a01e-2a44117c927f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.model_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1caa50e-6b03-46fe-836f-146856425472",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Conv_Wav2vec(n_inp_features=16,\n",
    "             channels = [64, 64, 64, 64, 64], \n",
    "                 kernel_sizes=[9, 5, 5, 3, 3],\n",
    "                 strides=[4, 4, 4, 2, 2], \n",
    "                 dilations = [1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "37261fc6-83dc-4462-b11a-362aad114a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef torch.Size([2, 16, 4096])\n",
      "after torch.Size([2, 64, 1024])\n",
      "bef torch.Size([2, 64, 1024])\n",
      "after torch.Size([2, 64, 256])\n",
      "bef torch.Size([2, 64, 256])\n",
      "after torch.Size([2, 64, 64])\n",
      "bef torch.Size([2, 64, 64])\n",
      "after torch.Size([2, 64, 32])\n",
      "bef torch.Size([2, 64, 32])\n",
      "after torch.Size([2, 64, 16])\n",
      "torch.Size([2, 64, 16])\n"
     ]
    }
   ],
   "source": [
    "eeg = torch.ones([2, 64, 16, 4096])\n",
    "\n",
    "# block = Conv_block(in_channels=16, out_channels=64, kernel_size=9, stride=4)\n",
    "\n",
    "eeg_out_2 = encoder(eeg[:, 0])\n",
    "print(eeg_out_2.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0329c930-c21f-4319-91a4-18929fc3b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef torch.Size([2, 16, 4096])\n",
      "after torch.Size([2, 64, 1024])\n",
      "bef torch.Size([2, 64, 1024])\n",
      "after torch.Size([2, 64, 256])\n",
      "bef torch.Size([2, 64, 256])\n",
      "after torch.Size([2, 64, 64])\n",
      "bef torch.Size([2, 64, 64])\n",
      "after torch.Size([2, 64, 32])\n",
      "bef torch.Size([2, 64, 32])\n",
      "after torch.Size([2, 64, 16])\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "      Conv_block-1       [2, 64, 1024]           9,344           9,344\n",
      "      Conv_block-2        [2, 64, 256]          20,608          20,608\n",
      "      Conv_block-3         [2, 64, 64]          20,608          20,608\n",
      "      Conv_block-4         [2, 64, 32]          12,416          12,416\n",
      "      Conv_block-5         [2, 64, 16]          12,416          12,416\n",
      "=======================================================================\n",
      "Total params: 75,392\n",
      "Trainable params: 75,392\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(summary(encoder, eeg[:, 0], show_input=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edd6f5-58f8-465d-be6e-c11b59c4f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones([2, 64, 128, 32])\n",
    "batch, ch, emb, time = x.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13a588c6-85f6-4f6d-a3e9-aec303a985cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 4096])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c13c75-b160-43f8-b269-9c3778b1f291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf83a5e-ec9d-4ee4-8b2c-869e3aefc4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795ff0e-da97-494e-a5eb-39ce70f105fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_custom(nn.Module):\n",
    "    def __init__(self,\n",
    "                 sequence_length=16, \n",
    "                 embed_dim=512,\n",
    "                 num_heads=4,\n",
    "                 mlp_ratio=2,\n",
    "                 attn_dropout=0.1,\n",
    "                 num_layers=1,                \n",
    "                 mlp_dropout=0.1,                 \n",
    "                 n_classes=1, \n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.class_embed = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "        self.pe = nn.Parameter(torch.zeros(1, sequence_length + 1, embed_dim), requires_grad=True)\n",
    "        \n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                            nhead=num_heads, \n",
    "                                                            dim_feedforward=embed_dim*mlp_ratio, \n",
    "                                                            dropout=attn_dropout, \n",
    "                                                            activation='relu', \n",
    "                                                            batch_first=True)\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*mlp_ratio),\n",
    "            nn.Dropout(p=mlp_dropout), \n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim*mlp_ratio, 1), )\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x.shape - [batch, time , features]\n",
    "        \n",
    "        \"\"\"\n",
    "        batch, time, features = x.size()\n",
    "        \n",
    "        # repeating like batch size and add to batch.\n",
    "        class_token = self.class_embed.expand(batch, -1, -1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "        \n",
    "        # add positional embeddings\n",
    "        x += self.pe\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # take first vector -> normalize -> mlp into \n",
    "        x_cls = x[:, 0]\n",
    "        x_cls = self.mlp_head(self.norm(x_cls))\n",
    "        return x_cls\n",
    "    \n",
    "    \n",
    "transformer_settings = dict(sequence_length=16, \n",
    "                             embed_dim=256,\n",
    "                             num_heads=2,\n",
    "                             num_layers=2, \n",
    "                             mlp_ratio=2,\n",
    "                             attn_dropout=0.3,                \n",
    "                             mlp_dropout=0.5,                 \n",
    "                             n_classes=1)\n",
    "vit = ViT_custom(**transformer_settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_torch",
   "language": "python",
   "name": "myenv_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
